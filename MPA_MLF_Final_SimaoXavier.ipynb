{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simaoXVS/MPC-MLF/blob/main/MPA_MLF_Final_SimaoXavier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MPA-MLF Final Project\n",
        "###Simão Xavier 263873"
      ],
      "metadata": {
        "id": "OHDphVWrs_kj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsdtmgzM3BJT"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD8PbnN84NJy"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "uQ57-bu2McvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4CytgpM3BJV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "import time\n",
        "import random # for torch seed\n",
        "import os # for torch seed\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, AdamW, RMSprop # optmizers\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau # Learning rate schedulers\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "import timm\n",
        "import wandb\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "from skimage.color import rgba2rgb\n",
        "from skimage import data\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYo0JN-63BJW"
      },
      "outputs": [],
      "source": [
        "print('timm version', timm.__version__)\n",
        "print('torch version', torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL5uz21x3BJY"
      },
      "source": [
        "# Read"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf train_data_unlabeled/"
      ],
      "metadata": {
        "id": "u-rC0m0YqjYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y27a8r9v7oJz"
      },
      "outputs": [],
      "source": [
        "!unzip train_data_unlabeled.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjaaNWct7rWd"
      },
      "outputs": [],
      "source": [
        "!unzip test_data_unlabeled.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXrVt11v5CAu"
      },
      "outputs": [],
      "source": [
        "train_data_path = Path(\"train_data_unlabeled/\")\n",
        "test_data_path = Path(\"test_data_unlabeled/\")\n",
        "train_labels_csv = \"y_train.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3HRg7BI82HA"
      },
      "outputs": [],
      "source": [
        "train_images = list(train_data_path.iterdir())\n",
        "test_images = list(test_data_path.iterdir())\n",
        "pd.read_csv(train_labels_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rv3bgVrAuV5"
      },
      "outputs": [],
      "source": [
        "plt.imshow(plt.imread(train_images[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix19n-jepl2Z"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "root_dir = '.'  # Update this path\n",
        "dataset_dir = os.path.join(root_dir, 'dataset')\n",
        "images_dir = os.path.join(root_dir, 'train_data_unlabeled')\n",
        "labels_file = os.path.join(root_dir, 'y_train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ-GODsy3BJc"
      },
      "source": [
        "# CFG\n",
        "\n",
        "This class enables easy configuration to simulate and understand results.\n",
        "\n",
        "You can tweak some parameters and see how they impact on the metrics, plots, and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RFBi7rQ3BJc"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "  DEBUG = False # True False\n",
        "\n",
        "  ### input: not configurable\n",
        "  IMG_HEIGHT = 45\n",
        "  IMG_WIDTH = 51\n",
        "  N_CLASS = 4\n",
        "\n",
        "  ### split train and validation sets\n",
        "  split_fraction = 1.0\n",
        "\n",
        "  ### model\n",
        "  # model_name = \"resnet34\"\n",
        "  model_name = \"efficientnet_b1_pruned\"\n",
        "  pretrained = True\n",
        "\n",
        "  ### training\n",
        "  print_freq = 100\n",
        "  BATCH_SIZE = 32\n",
        "  N_EPOCHS = 3 if DEBUG else 80\n",
        "\n",
        "  ### set only one to True\n",
        "  save_best_loss = False\n",
        "  save_best_accuracy = True\n",
        "\n",
        "  ### optimizer\n",
        "#   optimizer = 'adam'\n",
        "  optimizer = 'adamw'\n",
        "  # optimizer = 'rmsprop'\n",
        "\n",
        "  LEARNING_RATE = 1e-3\n",
        "\n",
        "  weight_decay = 0.1 # for adamw\n",
        "  l2_penalty = 0.01 # for RMSprop\n",
        "  rms_momentum = 0 # for RMSprop\n",
        "\n",
        "  ### learning rate scheduler (LRS)\n",
        "  scheduler = 'ReduceLROnPlateau'\n",
        "#   scheduler = 'CosineAnnealingLR'\n",
        "  plateau_factor = 0.5\n",
        "  plateau_patience = 3\n",
        "  cosine_T_max = 4\n",
        "  cosine_eta_min = 1e-8\n",
        "  verbose = True\n",
        "\n",
        "  ### processing\n",
        "  resize = True\n",
        "  resize_value = (64, 64)\n",
        "  padding = True\n",
        "\n",
        "  ### augmentations\n",
        "  probability = 0.6\n",
        "  random_horizontal_flip = True\n",
        "  random_vertical_flip = True\n",
        "\n",
        "  random_seed = 88\n",
        "\n",
        "cfg_dict = dict(CFG.__dict__)\n",
        "wandb_dict = dict()\n",
        "for key, value in cfg_dict.items():\n",
        "    if not key.startswith('__'):\n",
        "      wandb_dict.update({key: value})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new run\n",
        "username = \"simaoxavier\"\n",
        "wandb.init(project='DopplerClassifier', entity=username, config=wandb_dict)"
      ],
      "metadata": {
        "id": "8jYVf0-lNxXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZVHrWqE3BJg"
      },
      "outputs": [],
      "source": [
        "# detect and define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX7yPGzO3BJi"
      },
      "outputs": [],
      "source": [
        "# for reproducibility\n",
        "def seed_torch(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed = CFG.random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "ocWHDOY8seaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf dataset"
      ],
      "metadata": {
        "id": "GZwmMZeyu9Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNtoT2qLQ0Ow"
      },
      "outputs": [],
      "source": [
        "# Load labels\n",
        "df = pd.read_csv(labels_file)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['target'], random_state=42)\n",
        "\n",
        "# Function to create directories and move files\n",
        "def prepare_data(df, dest_folder):\n",
        "    dest_dir = os.path.join(dataset_dir, dest_folder)\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        file_path = os.path.join(images_dir, f\"img_{str(row['id']+1)}.png\")\n",
        "        if not os.path.exists(file_path):\n",
        "          print(f\"img_{row['id']+1}.png not found in {images_dir}!\")\n",
        "          continue\n",
        "        label_dir = os.path.join(dest_dir, str(row['target']))\n",
        "        if not os.path.exists(label_dir):\n",
        "            os.makedirs(label_dir)\n",
        "        shutil.copy(file_path, os.path.join(label_dir, f\"img_{str(row['id']+1)}.png\"))\n",
        "\n",
        "# Submission run\n",
        "if CFG.split_fraction == 1.0:\n",
        "  prepare_data(df, 'train')\n",
        "  prepare_data(val_df, 'val')\n",
        "# Prepare train and validation directories\n",
        "else:\n",
        "  prepare_data(train_df, 'train')\n",
        "  prepare_data(val_df, 'val')\n",
        "\n",
        "print(\"Data has been split and placed into train and val folders.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMruxAWjQ4wY"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_transform(target_size=(64, 64), thresh=0.4, mean=0.0, std=0.1, with_augmentations=False, probability=CFG.probability):\n",
        "    def remove_alpha_channel(img):\n",
        "        # Convert RGBA to RGB\n",
        "        if img.mode == 'RGBA':\n",
        "            img = img.convert('RGB')\n",
        "        return img\n",
        "\n",
        "    def pad_if_needed(img):\n",
        "        width, height = img.size\n",
        "        max_side = max(width, height)\n",
        "        padding = (0, 0, max_side - width, max_side - height)  # left, top, right, bottom\n",
        "        return transforms.functional.pad(img, padding, padding_mode='constant', fill=0)\n",
        "\n",
        "    def to_binary(img):\n",
        "        # Convert image to grayscale\n",
        "        gray_img = img.convert('L')\n",
        "        # Apply threshold to convert image to binary\n",
        "        return gray_img.point(lambda x: 255 if x > thresh * 255 else 0).convert('RGB')\n",
        "\n",
        "    if with_augmentations:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Lambda(remove_alpha_channel),  # Remove alpha channel if present\n",
        "            transforms.Lambda(pad_if_needed),         # Pad to make square\n",
        "            transforms.Resize(target_size),           # Resize to the target size\n",
        "            transforms.ToTensor(),                     # Convert to tensor\n",
        "            transforms.RandomHorizontalFlip(probability),  # Random horizontal flip\n",
        "            transforms.RandomVerticalFlip(probability),  # Random horizontal flip\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Lambda(remove_alpha_channel),  # Remove alpha channel if present\n",
        "            transforms.Lambda(pad_if_needed),         # Pad to make square\n",
        "            transforms.Resize(target_size),           # Resize to the target size\n",
        "            transforms.ToTensor(),                    # Convert to tensor\n",
        "        ])\n",
        "    return transform\n",
        "\n",
        "# Transforms\n",
        "train_transform = get_transform(with_augmentations=True)\n",
        "valid_transform = get_transform()\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_dir, 'train'), transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(dataset_dir, 'val'), transform=valid_transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "###############\n",
        "img = Image.open('dataset/train/3/img_10005.png')  # Make sure to load an actual file path\n",
        "\n",
        "# Get the transform function\n",
        "debug_transform = get_transform(with_augmentations=True, probability=1.0)\n",
        "\n",
        "# Apply the transform\n",
        "transformed_img = debug_transform(img)\n",
        "\n",
        "# If you want to visualize the result using matplotlib\n",
        "plt.imshow(np.transpose(transformed_img.numpy(), (1, 2, 0)))\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zaPkCVc3BJo"
      },
      "source": [
        "# Transfer Learning: timm\n",
        "\n",
        "- [timm.create_model and timm.list_models](https://huggingface.co/docs/timm/reference/models)\n",
        "- [timm docs](https://huggingface.co/docs/timm/feature_extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLuIb_XO3BJo"
      },
      "outputs": [],
      "source": [
        "class DopplerModel(nn.Module):\n",
        "    def __init__(self, model_name = CFG.model_name, pretrained = CFG.pretrained):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.cnn = timm.create_model(self.model_name, pretrained = pretrained, num_classes = CFG.N_CLASS)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqWvbIjn3BJo"
      },
      "outputs": [],
      "source": [
        "CFG.model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcVqyI5-3BJr"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "[torch optimizer documentation](https://pytorch.org/docs/stable/optim.html#)\n",
        "\n",
        "Function to get the optimizer to be used (can be tuned in CFG class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA8RWEc83BJs"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(lr = CFG.LEARNING_RATE):\n",
        "\n",
        "  if CFG.optimizer == 'adam':\n",
        "      optimizer = Adam(model.parameters(), lr=lr, weight_decay = CFG.weight_decay, amsgrad = False)\n",
        "\n",
        "  elif CFG.optimizer == 'adamw':\n",
        "      optimizer = AdamW(model.parameters(), lr = lr, weight_decay = CFG.weight_decay)\n",
        "\n",
        "  elif CFG.optimizer == 'rmsprop':\n",
        "      optimizer = RMSprop(model.parameters(), lr = lr, weight_decay = CFG.l2_penalty, momentum = CFG.rms_momentum)\n",
        "\n",
        "  else:\n",
        "      print('Optimizer is not defined')\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0fpeRnh3BJt"
      },
      "source": [
        "# LR Scheduler\n",
        "\n",
        "[torch LRS documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
        "\n",
        "Function to get the Learning Rate Scheduler to be used (can be tuned in CFG class).\n",
        "\n",
        "`torch.optim.lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs.\n",
        "\n",
        "`torch.optim.lr_scheduler.ReduceLROnPlateau` allows dynamic learning rate reducing based on some validation measurements.\n",
        "\n",
        "Learning rate scheduling should be applied after optimizer’s update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Ji42ev3BJv"
      },
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer):\n",
        "\n",
        "  if CFG.scheduler=='ReduceLROnPlateau':\n",
        "      scheduler = ReduceLROnPlateau(optimizer, mode='max', factor = CFG.plateau_factor, patience = CFG.plateau_patience, verbose = CFG.verbose)\n",
        "\n",
        "  elif CFG.scheduler=='CosineAnnealingLR':\n",
        "      scheduler = CosineAnnealingLR(optimizer, T_max = CFG.cosine_T_max, eta_min = CFG.cosine_eta_min)\n",
        "\n",
        "  else:\n",
        "      print('LR Scheduler is not defined')\n",
        "\n",
        "  return scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xruWGdD_3BJw"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAngRfre3BJx"
      },
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, criterion, optmizer, device):\n",
        "  # switch to train mode\n",
        "  model.train()\n",
        "\n",
        "\n",
        "  size = len(train_loader.dataset)\n",
        "  num_batches = len(train_loader)\n",
        "\n",
        "  loss, correct = 0, 0\n",
        "\n",
        "  ################################# train #################################\n",
        "\n",
        "  for batch, (X, y) in enumerate(train_loader):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    device = torch.device(device)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # compute predictions and loss\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(X)\n",
        "    loss = criterion(pred, y.long().squeeze())\n",
        "    current = batch * len(X)\n",
        "\n",
        "    # Backpropagation: only in train function, not done in validation function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # sum correct predictions\n",
        "    y_pred, y_true = torch.argmax(pred, axis=1), y.long().squeeze()\n",
        "    correct += (y_pred == y_true).type(torch.float).sum().item()\n",
        "\n",
        "    end = time.time()\n",
        "    time_delta = np.round(end - start, 3)\n",
        "\n",
        "    # log\n",
        "    loss, current = np.round(loss.item(), 5), batch * len(X)\n",
        "\n",
        "  # metrics: calculate accuracy and loss for epoch (all batches)\n",
        "  correct /= size # epoch accuracy\n",
        "  loss /= num_batches # epoch loss\n",
        "\n",
        "  print(f\"Train: Accuracy: {(100*correct):>0.2f}%, Avg loss: {loss:>5f} \\n\")\n",
        "\n",
        "  return loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1R3yz5f3BJy"
      },
      "source": [
        "# Validation function\n",
        "\n",
        "[torch.no_grad documentation](https://pytorch.org/docs/stable/generated/torch.no_grad.html)\n",
        "\n",
        "Use `torch.no_grad` when you are sure you will not call `Tensor.backward()`. It reduces memory and time consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQphupnR3BJy"
      },
      "outputs": [],
      "source": [
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "  model.eval()\n",
        "\n",
        "  size = len(valid_loader.dataset)\n",
        "  num_batches = len(valid_loader)\n",
        "\n",
        "  loss, correct = 0, 0\n",
        "\n",
        "  ################################# validation #################################\n",
        "\n",
        "  with torch.no_grad(): # disable gradients\n",
        "    for batch, (X, y) in enumerate(valid_loader):\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      device = torch.device(device)\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # compute predictions and loss\n",
        "      pred = model(X)\n",
        "      loss = criterion(pred, y.long().squeeze())\n",
        "      current = batch * len(X)\n",
        "\n",
        "      # sum correct predictions\n",
        "      y_pred, y_true = torch.argmax(pred, axis=1), y.long().squeeze()\n",
        "      correct += (y_pred == y_true).type(torch.float).sum().item()\n",
        "\n",
        "      end = time.time()\n",
        "      time_delta = np.round(end - start, 3)\n",
        "\n",
        "      # log\n",
        "      loss, current = np.round(loss.item(), 5), batch * len(X)\n",
        "\n",
        "  # metrics: calculate accuracy and loss for epoch (all batches)\n",
        "  correct /= size # epoch accuracy\n",
        "  loss /= num_batches # epoch loss\n",
        "\n",
        "  print(f\"Valid: Accuracy: {(100*correct):>0.2f}%, Avg loss: {loss:>5f} \\n\")\n",
        "\n",
        "  return loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2NalKsC3BJ0"
      },
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK7pbGV33BJ0"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# instantiate model\n",
        "device = torch.device(device)\n",
        "model = DopplerModel().to(device) # move the model to GPU before constructing optimizers for it\n",
        "\n",
        "print('\\n ******************************* Using backbone: ', CFG.model_name, \" ******************************* \\n\")\n",
        "\n",
        "# define optimizer\n",
        "optimizer = get_optimizer(lr = CFG.LEARNING_RATE)\n",
        "\n",
        "# define scheduler\n",
        "scheduler = get_scheduler(optimizer)\n",
        "\n",
        "OUTPUT_PATH = './'\n",
        "\n",
        "train_dataset = train_dataset\n",
        "valid_dataset = val_dataset\n",
        "train_dataloader = train_loader\n",
        "valid_dataloader = val_loader\n",
        "\n",
        "\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "valid_loss_history = []\n",
        "valid_acc_history = []\n",
        "LR_history = []\n",
        "\n",
        "best_loss = np.inf\n",
        "best_epoch_loss = 0\n",
        "best_acc = 0\n",
        "best_epoch_acc = 0\n",
        "\n",
        "print('Starting Training...\\n')\n",
        "\n",
        "start_train_time = time.time()\n",
        "\n",
        "for epoch in range(0, CFG.N_EPOCHS):\n",
        "  print(f\"\\n-------------------------------   Epoch {epoch + 1}   -------------------------------\\n\")\n",
        "  start_epoch_time = time.time()\n",
        "\n",
        "  # train\n",
        "  train_loss, train_acc = train_fn(train_dataloader, model, loss_fn, optimizer, device)\n",
        "  train_loss_history.append(train_loss)\n",
        "  train_acc_history.append(train_acc)\n",
        "\n",
        "  # validation\n",
        "  valid_loss, valid_acc = valid_fn(valid_dataloader, model, loss_fn, device)\n",
        "  valid_loss_history.append(valid_loss)\n",
        "  valid_acc_history.append(valid_acc)\n",
        "\n",
        "  # apply LR scheduler after each epoch\n",
        "  if isinstance(scheduler, ReduceLROnPlateau):\n",
        "      scheduler.step(valid_loss)\n",
        "\n",
        "  elif isinstance(scheduler, CosineAnnealingLR):\n",
        "      scheduler.step()\n",
        "\n",
        "  # save LR value to plot later\n",
        "  for param_group in optimizer.param_groups:\n",
        "    LR_history.append(param_group['lr'])\n",
        "\n",
        "  # Log metrics\n",
        "  wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": valid_loss, \"val_accuracy\": valid_acc, \"lr\": LR_history[-1],\"epoch\": epoch})\n",
        "\n",
        "  # save validation loss if it was improved (reduced)\n",
        "  if valid_loss < best_loss:\n",
        "    best_epoch_loss = epoch + 1\n",
        "    best_loss = valid_loss\n",
        "    if CFG.save_best_loss:\n",
        "      # save the model's weights and biases only if CFG.save_best_loss == True\n",
        "      torch.save(model.state_dict(), OUTPUT_PATH + f\"DopplerModel_ep{best_epoch_loss}.pth\")\n",
        "\n",
        "  # save validation accuracy if it was improved (increased)\n",
        "  if valid_acc > best_acc:\n",
        "    best_epoch_acc = epoch + 1\n",
        "    best_acc = valid_acc\n",
        "    if CFG.save_best_accuracy:\n",
        "      # save the model's weights and biases only if CFG.save_best_accuracy == True\n",
        "      torch.save(model.state_dict(), OUTPUT_PATH + f\"DopplerModel_ep{best_epoch_acc}.pth\")\n",
        "      wandb.save(f\"DopplerModel_ep{best_epoch_acc}.pth\")\n",
        "\n",
        "  # Save all epochs\n",
        "  # torch.save(model.state_dict(), OUTPUT_PATH + f\"DopplerModel_ep{epoch + 1}.pth\")\n",
        "  # wandb.save(f\"DopplerModel_ep{epoch + 1}.pth\")\n",
        "\n",
        "  end_epoch_time = time.time()\n",
        "  time_delta = np.round(end_epoch_time - start_epoch_time, 3)\n",
        "  print(\"\\n\\nEpoch Elapsed Time: {} s\".format(time_delta))\n",
        "\n",
        "end_train_time = time.time()\n",
        "print(\"\\n\\nTotal Elapsed Time: {} min\".format(np.round((end_train_time - start_train_time)/60, 3)))\n",
        "print(\"Done!\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayKcySzn3BKD"
      },
      "source": [
        "# Plot Epochs\n",
        "\n",
        "Plot Train and Validation Loss and Accuracy for each Epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUkw_j233BKE"
      },
      "outputs": [],
      "source": [
        "print('Best validation loss: ', round(best_loss, 5))\n",
        "print('Best epoch (loss criteria in validation): ', best_epoch_loss)\n",
        "print('\\n')\n",
        "print('Best validation accuracy: ', round(best_acc, 5))\n",
        "print('Best epoch (accuracy criteria in validation): ', best_epoch_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0DRVULE3BKF"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (18, 8))\n",
        "fig.suptitle('Epoch Results', fontsize = 18)\n",
        "\n",
        "abscissa = np.arange(1, CFG.N_EPOCHS + 1, 1)\n",
        "\n",
        "# x_ticks according to CFG.N_EPOCHS for better visuailzation\n",
        "if CFG.N_EPOCHS <= 20:\n",
        "  x_ticks = np.arange(1, CFG.N_EPOCHS + 1, 1)\n",
        "else:\n",
        "  x_ticks = np.arange(1, CFG.N_EPOCHS + 1, int(CFG.N_EPOCHS/20) + 1)\n",
        "\n",
        "# Loss plot\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "ax1.plot(abscissa, train_loss_history, label='Training', color = 'black')\n",
        "ax1.plot(abscissa, valid_loss_history, label='Validation', color = 'red')\n",
        "plt.xticks(x_ticks)\n",
        "plt.axhline(0, linestyle = 'dashed', color = 'grey')\n",
        "plt.axvline(best_epoch_loss, linestyle = 'dashed', color = 'blue', label = 'Best val loss: ep ' + str(best_epoch_loss))\n",
        "plt.axvline(best_epoch_acc, linestyle = 'dashed', color = 'green', label = 'Best val acc: ep ' + str(best_epoch_acc))\n",
        "plt.title(\"Loss\")\n",
        "ax1.legend(frameon=False);\n",
        "\n",
        "# Accuracy plot\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "ax2.plot(abscissa, train_acc_history, label='Training', color = 'black')\n",
        "ax2.plot(abscissa, valid_acc_history, label='Validation', color = 'red')\n",
        "plt.xticks(x_ticks)\n",
        "plt.axhline(0.99, linestyle = 'dashed', color = 'grey')\n",
        "plt.axvline(best_epoch_loss, linestyle = 'dashed', color = 'blue', label = 'Best val loss: ep ' + str(best_epoch_loss))\n",
        "plt.axvline(best_epoch_acc, linestyle = 'dashed', color = 'green', label = 'Best val acc: ep ' + str(best_epoch_acc))\n",
        "plt.title(\"Accuracy\")\n",
        "ax2.legend(frameon=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoIFpTXc3BKF"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (14, 8))\n",
        "\n",
        "abscissa = np.arange(1, CFG.N_EPOCHS + 1, 1)\n",
        "\n",
        "# x_ticks according to CFG.N_EPOCHS for better visuailzation\n",
        "if CFG.N_EPOCHS <= 20:\n",
        "  x_ticks = np.arange(1, CFG.N_EPOCHS + 1, 1)\n",
        "else:\n",
        "  x_ticks = np.arange(1, CFG.N_EPOCHS + 1, int(CFG.N_EPOCHS/20) + 1)\n",
        "\n",
        "# LR plot\n",
        "plt.plot(abscissa, LR_history, label='LR', color = 'orange')\n",
        "plt.xticks(x_ticks)\n",
        "plt.axhline(CFG.LEARNING_RATE, linestyle = 'dashed', color = 'grey')\n",
        "plt.axhline(0, linestyle = 'dashed', color = 'grey')\n",
        "plt.axvline(best_epoch_loss, linestyle = 'dashed', color = 'blue', label = 'Best val loss: ep ' + str(best_epoch_loss))\n",
        "plt.axvline(best_epoch_acc, linestyle = 'dashed', color = 'green', label = 'Best val acc: ep ' + str(best_epoch_acc))\n",
        "plt.title(f\"Learning Rate vs Epochs: {CFG.scheduler}\", fontsize = 16, color = 'orange')\n",
        "plt.legend(frameon=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xSXPuJL3BKG"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test"
      ],
      "metadata": {
        "id": "sFWT-0KUDVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test/unlabeled"
      ],
      "metadata": {
        "id": "H6LTzUtuC9XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = list(Path(\"test_data_unlabeled/\").iterdir())\n",
        "for file in test_list:\n",
        "  id = (file.stem).split('_')[-1]\n",
        "  shutil.copy(str(file),f\"test/unlabeled/{id.zfill(6)}.png\")\n",
        "\n",
        "list(Path(\"test/unlabeled/\").iterdir())"
      ],
      "metadata": {
        "id": "eoZ8v5qgPvQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR17J2Qz3BKG"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x), axis=1)[:, None]\n",
        "\n",
        "def inference(test_loader, model):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "\n",
        "    model = DopplerModel().to(device)\n",
        "\n",
        "    if CFG.save_best_loss: # load model with best validation loss\n",
        "      model.load_state_dict(torch.load(OUTPUT_PATH + f\"DopplerModel_ep{best_epoch_loss}.pth\"))\n",
        "    else: # load model with best validation accuracy\n",
        "      model.load_state_dict(torch.load(OUTPUT_PATH + f\"DopplerModel_ep{best_epoch_acc}.pth\"))\n",
        "\n",
        "    # load model from specific epoch\n",
        "    # model.load_state_dict(torch.load(OUTPUT_PATH + f\"DopplerModel_ep60.pth\"))\n",
        "\n",
        "    # disable gradients for inference\n",
        "    with torch.no_grad():\n",
        "      for batch, (X, _) in enumerate(test_loader):\n",
        "\n",
        "        ################################# inference #################################\n",
        "        start = time.time()\n",
        "        current = batch * len(X)\n",
        "\n",
        "        X = X.to(device)\n",
        "\n",
        "        # compute predictions\n",
        "        pred = model(X)\n",
        "        # softmax\n",
        "        y_pred = softmax(pred.detach().cpu().numpy()) # convert tensor to numpy and apply softmax\n",
        "        y_pred = np.argmax(y_pred, axis = 1) # take the indice of the max value (higher probability: predicted class)\n",
        "\n",
        "        # store results\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "        # log\n",
        "        end = time.time()\n",
        "        time_delta = np.round(end - start, 5)\n",
        "\n",
        "    test_predictions = np.concatenate(predictions, axis = 0) # join sequence of arrays along axis 0\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnbiif6A3BKH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# instantiate Inference Dataset class (create inference Dataset)\n",
        "test_transform = get_transform()\n",
        "inference_dataset = datasets.ImageFolder(root=os.path.join('test'), transform=test_transform)\n",
        "\n",
        "# Data loaders\n",
        "inference_dataloader = DataLoader(inference_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EClyTmJ13BKH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# run inference\n",
        "predictions = inference(inference_dataloader, model)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions)"
      ],
      "metadata": {
        "id": "nDT5Gu5qD7nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VCZM7893BKI"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WauVbghH3BKI"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv(\"submission_example.csv\")\n",
        "submission[\"target\"] = predictions\n",
        "\n",
        "submission.to_csv(OUTPUT_PATH + 'submission.csv', index = False)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJTMssyt3BKI"
      },
      "source": [
        "# Check predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K54Bqfqb3BKJ"
      },
      "outputs": [],
      "source": [
        "# check some predictions\n",
        "\n",
        "fig = plt.figure(figsize = (12, 12))\n",
        "fig.suptitle('Visualizing Predictions', fontsize = 24)\n",
        "\n",
        "# define a range of predictions to plot\n",
        "begin = 130\n",
        "end = begin + 20\n",
        "\n",
        "test_list = list(Path(\"test/unlabeled/\").iterdir())\n",
        "\n",
        "for i in range(begin, end):\n",
        "  img = plt.imread(test_list[i])\n",
        "\n",
        "  plt.subplot(4, 5, i + 1 - begin) # 4 rows and 5 columns plot\n",
        "  label = str(submission.loc[i, 'target'])\n",
        "  plt.title(\"Predicted label: \" + label, color=\"red\") # write label in each image title\n",
        "  plt.imshow(np.squeeze(img), cmap='gray') # plot image\n",
        "  plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}