#author: simao xavier

# Step 1: Data Preprocessing
import matplotlib
import pandas as pd

# Load the dataset
data = pd.read_csv('Data_prepared_final.csv')

print(data.head())  # Display the first few rows of the dataset to verify it was read correctly
print(data.info())  # Display information about the dataset

# Check for missing values
missing_values = data.isnull().sum()
print("Missing values:\n", missing_values)

# Explore data distribution and summary statistics
summary_statistics = data.describe()
print("Summary statistics:\n", summary_statistics)

# Handle missing values if necessary
# No missing values on the provided excel (missing_values = 0) and also checked manually the .csv

# Handle duplicates by aggregating data
data_aggregated = data.groupby('time [s]').mean().reset_index()

# Sort the aggregated data by time
data_aggregated_sorted = data_aggregated.sort_values(by='time [s]')

# Step 2: Feature Engineering
# No specific feature engineering is mentioned in the task description

# Step 3: Data Visualization
import matplotlib
import matplotlib.pyplot as plt
plt.rcdefaults()
matplotlib.use('TkAgg')

# Visualize distributions of features

#With no time order and no removal of duplicates (data)
for column in data_aggregated_sorted.columns[2:4]:
    # Create the plot
    plt.plot(data['time [s]'], data[column])
    plt.xlabel('Time [s]')
    plt.ylabel(column)  # Use the column name as the y-axis label
    plt.title(f'{column} over Time')
    plt.savefig(f'{column}_over_time_no_order.png')  # Save the plot as a PNG file with a unique name
    plt.clf()

#after pre-processing (data_aggregated_sorted)
for column in data_aggregated_sorted.columns[2:]:
    # Create the plot
    plt.plot(data_aggregated_sorted['time [s]'], data_aggregated_sorted[column])
    plt.xlabel('Time [s]')
    plt.ylabel(column)  # Use the column name as the y-axis label
    plt.title(f'{column} over Time')
    plt.savefig(f'{column}_over_time.png')  # Save the plot as a PNG file with a unique name
    plt.clf()

'''
# Step 4: Model Selection and Training
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split the data into features (X) and target variable (y)
X = data.drop('number_of_transmitters', axis=1)
y = data['number_of_transmitters']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Step 5: Model Evaluation
# Evaluate the classifier on the test set
y_pred = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
'''

# Step 6: Hyperparameter Tuning
# If needed, optimize the hyperparameters of the Random Forest classifier

# Step 7: Model Deployment
# Deploy the trained model for making predictions on new data
# I will leave this for the final project (maybe needed there)

# Step 8: Documentation and Reporting
# Document the entire workflow, including code comments, and prepare a report summarizing the findings.
# On other files

# Conclusion:
# After reviewing some/all plots generated by this code is easy to see that we have 7 wireless transmitters.
